{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Flatten, Activation\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.optimizers import SGD, Adadelta\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import csv\n",
    "import sys\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from csv import reader\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "global MAX_SEQUENCE_LENGTH, MAX_NB_WORDS , EMBEDDING_DIM, VALIDATION_SPLIT, DROP_OUT, Nb_EPOCH, BATCH_SIZE, Classes \n",
    "global Classes, DROP_OUT, EMBEDDING_DIM, Nb_EPOCH, FILENAME, TEXT_DATA_DIR\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000000\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 200\n",
    "VALIDATION_SPLIT = 0.1\n",
    "DROP_OUT = 0.3\n",
    "Nb_EPOCH = 3\n",
    "BATCH_SIZE = 10\n",
    "Classes = 2\n",
    "\n",
    "parameters = {\n",
    "\"classes\" : [2],\n",
    "#\"batches\" : [10, 20, 50, 100],\n",
    "#\"epochs\": [1, 10, 25, 50, 100], \n",
    "#\"dropout_rate\" : [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "#\"embedding_dimension\" : [25, 50, 100, 200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty\n",
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = '/Users/suzy/Documents/git/twitter-sentiment-analysis/glove/'\n",
    "FILENAME = 'glove.twitter.27B.' + str(EMBEDDING_DIM) + 'd.txt'\n",
    "TEXT_DATA_DIR = '/Users/suzy/Documents/GuidedStudy/author/dataset/Author_14.csv'\n",
    "global embeddings\n",
    "embeddings = {}\n",
    "fname = os.path.join(GLOVE_DIR, FILENAME)\n",
    "f = codecs.open(fname, 'r', encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    if(len(values) == 0):\n",
    "        print('Empty')\n",
    "    else:\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "print('Processing text dataset')\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "f = codecs.open(TEXT_DATA_DIR, 'r', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 238 texts.\n"
     ]
    }
   ],
   "source": [
    "r = reader(TEXT_DATA_DIR)\n",
    "i = 0\n",
    "with open(TEXT_DATA_DIR,'rb') as csvfile:\n",
    "    spamreader = reader(csvfile,delimiter=',')\n",
    "    raw_data_list= list(spamreader)\n",
    "    sorted(raw_data_list)\n",
    "\n",
    "for row in raw_data_list:\n",
    "    label_id = len(labels_index)\n",
    "    labels_index[int(row[0])] = label_id\n",
    "    labels.append(label_id)\n",
    "    texts.append(row[1])\n",
    "print('Found %s texts.' % len(texts))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 146181 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (238, 1000000)\n",
      "Shape of label tensor: (238, 18)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Training model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done compiling.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Training model.')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(                          # Layer 0, Start\n",
    "    input_dim=nb_words + 1,                   # Size to dictionary, has to be input + 1\n",
    "    output_dim=EMBEDDING_DIM,                 # Dimensions to generate\n",
    "    weights=[embedding_matrix],               # Initialize word weights\n",
    "    input_length=MAX_SEQUENCE_LENGTH))        # Define length to input sequences in the first layer\n",
    "\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(18))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Done compiling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1281565b93d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tart = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n\u001b[0m\u001b[1;32m      4\u001b[0m                     nb_epoch=1, batch_size=150)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#tart = time.time()\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "                    nb_epoch=1, batch_size=150)\n",
    "\n",
    "#print (\"Training Time : \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1379192\n"
     ]
    }
   ],
   "source": [
    "print(len((max(texts, key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "print('Processing text dataset')\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "label_origion =[]\n",
    "f = codecs.open(TEXT_DATA_DIR, 'r', encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 238 texts.\n"
     ]
    }
   ],
   "source": [
    "r = reader(TEXT_DATA_DIR)\n",
    "i = 0\n",
    "with open(TEXT_DATA_DIR,'rb') as csvfile:\n",
    "    spamreader = reader(csvfile,delimiter=',')\n",
    "    raw_data_list= list(spamreader)\n",
    "    sorted(raw_data_list)\n",
    "\n",
    "for row in raw_data_list:\n",
    "    label_origion.append(row[0])\n",
    "    label_id = len(labels_index)\n",
    "    labels_index[int(row[0])] = label_id\n",
    "    labels.append(label_id)\n",
    "    texts.append(row[1])\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10932',\n",
       " '11169',\n",
       " '11178',\n",
       " '1535',\n",
       " '1937',\n",
       " '2852',\n",
       " '31',\n",
       " '3710',\n",
       " '4281',\n",
       " '541',\n",
       " '6014',\n",
       " '6046',\n",
       " '6381',\n",
       " '698',\n",
       " '9726',\n",
       " '9772',\n",
       " '9872'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(label_origion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID= ['a','b','b','a','a','a']\n",
    "L =[]\n",
    "L_index ={}\n",
    "for id in ID:\n",
    "    label_id = len(L_index)\n",
    "    L_index[id] = label_id\n",
    "    L.append(label_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238, 11179)\n"
     ]
    }
   ],
   "source": [
    "label_o = to_categorical(np.asarray(label_origion))\n",
    "print(label_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
